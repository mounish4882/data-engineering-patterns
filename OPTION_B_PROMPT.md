# OPTION B: Build Manufacturing Data Platform from Scratch

## ğŸ¯ MISSION STATEMENT

Build a **production-grade, enterprise-scale manufacturing IoT data platform reference repository** from the ground up. This repository documents battle-tested patterns, troubleshooting playbooks, and architectural decisions from real multi-plant manufacturing environments processing billions of records.

---

## ğŸ‘¤ MY PROFILE & CONTEXT

### Who I Am
- **Senior Data Engineer** specializing in industrial IoT and manufacturing data
- **Scale**: Multi-plant deployments across domains, billions of records
- **Focus**: Real-time streaming, dimensional modeling, performance optimization
- **Tech Stack**: Kafka â†’ Spark â†’ Delta Lake â†’ Databricks

### What I Work On Daily

**1. Production Streaming Pipelines**
- Kafka with SASL_SSL authentication (troubleshooting auth failures regularly)
- Kafka Streams for preprocessing â†’ Spark for complex transformations
- Delta Lake with schema evolution challenges
- Checkpointing strategies and recovery

**2. Schema Management Nightmares**
- `DELTA_MERGE_INCOMPATIBLE_DATATYPE` errors in production
- MAP type evolution in `customExtension` fields
- 11-step progressive flattening strategy for nested IoT sensor data
- Handling unpredictable schema changes from 100+ machines

**3. Performance Optimization**
- Spark UI analysis to identify bottlenecks
- Eliminating unnecessary shuffles and explosions
- Optimizing pipelines processing billions of records
- Memory tuning, partition sizing, skew handling

**4. Dimensional Data Modeling**
- Multi-plant star/snowflake schemas
- SCD Type 2 for machine configurations
- Manufacturing fact tables (test results, sensor readings)
- Cross-plant dimension conformance

**5. Enterprise Integration**
- SAP system integration
- OPP (Offline Programming Platform) data processing
- Multiple data sources per plant
- Complex upstream dependencies

**6. DevOps & Deployment**
- Databricks Asset Bundles for multi-environment deployment
- HOCON configuration patterns
- GitHub Actions CI/CD pipelines
- Environment-specific configurations (dev/QA/prod)
- VasapiValidator test framework integration

---

## ğŸ“ REPOSITORY STRUCTURE

Build this structure from scratch:

```
manufacturing-data-platform/
â”‚
â”œâ”€â”€ README.md                                    # Platform overview & quick start
â”œâ”€â”€ ARCHITECTURE.md                              # High-level architecture decisions
â”œâ”€â”€ CONTRIBUTING.md                              # Contribution guidelines
â”œâ”€â”€ LICENSE                                      # MIT License
â”‚
â”œâ”€â”€ 01-authentication-security/                  # PRIORITY 1
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ kafka-sasl-ssl/
â”‚   â”‚   â”œâ”€â”€ README.md                           # Comprehensive SASL_SSL guide
â”‚   â”‚   â”œâ”€â”€ configs/
â”‚   â”‚   â”‚   â”œâ”€â”€ sasl-plain.yaml                # PLAIN mechanism
â”‚   â”‚   â”‚   â”œâ”€â”€ sasl-scram-256.yaml            # SCRAM-SHA-256
â”‚   â”‚   â”‚   â”œâ”€â”€ sasl-scram-512.yaml            # SCRAM-SHA-512
â”‚   â”‚   â”‚   â””â”€â”€ kerberos-gssapi.yaml           # GSSAPI for Kerberos
â”‚   â”‚   â”œâ”€â”€ databricks-integration/
â”‚   â”‚   â”‚   â”œâ”€â”€ secrets-management.py          # Secret scope integration
â”‚   â”‚   â”‚   â”œâ”€â”€ kafka-config-builder.py        # Dynamic config builder
â”‚   â”‚   â”‚   â””â”€â”€ connection-validator.py        # Test Kafka connectivity
â”‚   â”‚   â”œâ”€â”€ certificates/
â”‚   â”‚   â”‚   â”œâ”€â”€ truststore-setup.md            # Truststore/Keystore guide
â”‚   â”‚   â”‚   â”œâ”€â”€ cert-rotation-playbook.md      # Certificate rotation
â”‚   â”‚   â”‚   â””â”€â”€ cert-troubleshooting.md        # Common cert issues
â”‚   â”‚   â””â”€â”€ troubleshooting/
â”‚   â”‚       â”œâ”€â”€ auth-failure-playbook.md       # Step-by-step debugging
â”‚   â”‚       â”œâ”€â”€ common-errors.md               # Error codes & solutions
â”‚   â”‚       â””â”€â”€ connection-diagnostics.sh      # Diagnostic scripts
â”‚   â”‚
â”‚   â”œâ”€â”€ unity-catalog-authentication/
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”œâ”€â”€ service-principal-setup.md
â”‚   â”‚   â””â”€â”€ token-management.py
â”‚   â”‚
â”‚   â””â”€â”€ secrets-management/
â”‚       â”œâ”€â”€ azure-key-vault.md
â”‚       â”œâ”€â”€ databricks-secrets.md
â”‚       â””â”€â”€ secret-rotation-strategy.md
â”‚
â”œâ”€â”€ 02-schema-evolution/                         # PRIORITY 1
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ delta-type-conflicts/
â”‚   â”‚   â”œâ”€â”€ README.md                           # Handling incompatible types
â”‚   â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ schema_validator.py            # Pre-merge validation
â”‚   â”‚   â”‚   â”œâ”€â”€ type_conflict_resolver.py      # Auto-resolution strategies
â”‚   â”‚   â”‚   â”œâ”€â”€ compatibility_checker.py       # Schema compatibility matrix
â”‚   â”‚   â”‚   â””â”€â”€ merge_error_handler.py         # DELTA_MERGE_INCOMPATIBLE_DATATYPE
â”‚   â”‚   â”œâ”€â”€ examples/
â”‚   â”‚   â”‚   â”œâ”€â”€ int_to_long_promotion.py
â”‚   â”‚   â”‚   â”œâ”€â”€ string_to_int_cast.py
â”‚   â”‚   â”‚   â”œâ”€â”€ map_type_evolution.py
â”‚   â”‚   â”‚   â””â”€â”€ nested_struct_changes.py
â”‚   â”‚   â”œâ”€â”€ tests/
â”‚   â”‚   â”‚   â””â”€â”€ test_type_conflicts.py
â”‚   â”‚   â””â”€â”€ docs/
â”‚   â”‚       â”œâ”€â”€ error-reference.md             # All error codes explained
â”‚   â”‚       â””â”€â”€ resolution-strategies.md       # When to use each strategy
â”‚   â”‚
â”‚   â”œâ”€â”€ map-type-flattening/
â”‚   â”‚   â”œâ”€â”€ README.md                           # Your 11-step methodology
â”‚   â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ progressive_flattener.py       # Core flattening logic
â”‚   â”‚   â”‚   â”œâ”€â”€ custom_extension_handler.py    # MAP<STRING,STRING> handler
â”‚   â”‚   â”‚   â”œâ”€â”€ dynamic_key_discovery.py       # Discover unknown keys
â”‚   â”‚   â”‚   â”œâ”€â”€ type_inference.py              # Infer value types
â”‚   â”‚   â”‚   â””â”€â”€ optimization.py                # Avoid unnecessary explosions
â”‚   â”‚   â”œâ”€â”€ examples/
â”‚   â”‚   â”‚   â”œâ”€â”€ iot_sensor_flattening.py       # Manufacturing sensors
â”‚   â”‚   â”‚   â”œâ”€â”€ test_equipment_data.py         # Test result flattening
â”‚   â”‚   â”‚   â””â”€â”€ performance_comparison.ipynb   # Before/after benchmarks
â”‚   â”‚   â””â”€â”€ docs/
â”‚   â”‚       â”œâ”€â”€ 11-step-methodology.md         # Document your strategy
â”‚   â”‚       â””â”€â”€ optimization-techniques.md     # Shuffle elimination
â”‚   â”‚
â”‚   â”œâ”€â”€ schema-registry-integration/
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”œâ”€â”€ avro-schema-evolution.py
â”‚   â”‚   â””â”€â”€ compatibility-modes.md
â”‚   â”‚
â”‚   â””â”€â”€ production-patterns/
â”‚       â”œâ”€â”€ backward-compatible-changes.md
â”‚       â”œâ”€â”€ breaking-change-strategy.md
â”‚       â””â”€â”€ schema-versioning.md
â”‚
â”œâ”€â”€ 03-streaming-architecture/                   # PRIORITY 1
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ kafka-to-delta-lake/
â”‚   â”‚   â”œâ”€â”€ README.md                           # Production streaming pattern
â”‚   â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ streaming_pipeline.py          # Medallion architecture
â”‚   â”‚   â”‚   â”œâ”€â”€ checkpoint_manager.py          # Checkpoint strategies
â”‚   â”‚   â”‚   â”œâ”€â”€ watermark_handler.py           # Late data handling
â”‚   â”‚   â”‚   â”œâ”€â”€ backpressure_manager.py        # Rate limiting
â”‚   â”‚   â”‚   â””â”€â”€ monitoring.py                  # Metrics & health checks
â”‚   â”‚   â”œâ”€â”€ configs/
â”‚   â”‚   â”‚   â”œâ”€â”€ dev.yaml
â”‚   â”‚   â”‚   â”œâ”€â”€ qa.yaml
â”‚   â”‚   â”‚   â””â”€â”€ prod.yaml
â”‚   â”‚   â”œâ”€â”€ docker/
â”‚   â”‚   â”‚   â””â”€â”€ docker-compose.yml             # Local development
â”‚   â”‚   â””â”€â”€ docs/
â”‚   â”‚       â”œâ”€â”€ exactly-once-semantics.md
â”‚   â”‚       â”œâ”€â”€ checkpoint-recovery.md
â”‚   â”‚       â””â”€â”€ failure-scenarios.md
â”‚   â”‚
â”‚   â”œâ”€â”€ kstreams-preprocessing/
â”‚   â”‚   â”œâ”€â”€ README.md                           # Your hybrid architecture
â”‚   â”‚   â”œâ”€â”€ java-topology/
â”‚   â”‚   â”‚   â”œâ”€â”€ pom.xml
â”‚   â”‚   â”‚   â””â”€â”€ src/main/java/com/manufacturing/
â”‚   â”‚   â”‚       â”œâ”€â”€ PreprocessingTopology.java
â”‚   â”‚   â”‚       â”œâ”€â”€ SensorDataValidator.java
â”‚   â”‚   â”‚       â”œâ”€â”€ StatefulAggregator.java
â”‚   â”‚   â”‚       â”œâ”€â”€ MachineMetadataEnricher.java
â”‚   â”‚   â”‚       â””â”€â”€ serdes/
â”‚   â”‚   â”‚           â”œâ”€â”€ SensorEventSerde.java
â”‚   â”‚   â”‚           â””â”€â”€ AggregatedMetricsSerde.java
â”‚   â”‚   â”œâ”€â”€ spark-consumer/
â”‚   â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”‚   â”œâ”€â”€ consume_preprocessed.py        # Spark consumer
â”‚   â”‚   â”‚   â””â”€â”€ delta_writer.py
â”‚   â”‚   â”œâ”€â”€ deployment/
â”‚   â”‚   â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â”‚   â”‚   â””â”€â”€ kubernetes/
â”‚   â”‚   â”‚       â”œâ”€â”€ kstreams-deployment.yaml
â”‚   â”‚   â”‚       â””â”€â”€ spark-deployment.yaml
â”‚   â”‚   â””â”€â”€ docs/
â”‚   â”‚       â”œâ”€â”€ architecture-decisions.md      # Why KStreams + Spark?
â”‚   â”‚       â”œâ”€â”€ when-to-use-each.md            # Decision matrix
â”‚   â”‚       â””â”€â”€ stateful-operations.md
â”‚   â”‚
â”‚   â”œâ”€â”€ exactly-once-semantics/
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”œâ”€â”€ kafka-idempotent-producer.py
â”‚   â”‚   â”œâ”€â”€ transactional-writes.py
â”‚   â”‚   â””â”€â”€ delta-idempotent-writes.md
â”‚   â”‚
â”‚   â””â”€â”€ failure-recovery/
â”‚       â”œâ”€â”€ checkpoint-strategies.md
â”‚       â”œâ”€â”€ restart-procedures.md
â”‚       â””â”€â”€ data-consistency-checks.py
â”‚
â”œâ”€â”€ 04-dimensional-modeling/                     # PRIORITY 2
â”‚   â”œâ”€â”€ README.md                               # Manufacturing DWH patterns
â”‚   â”œâ”€â”€ star-schema-design/
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”œâ”€â”€ schemas/
â”‚   â”‚   â”‚   â”œâ”€â”€ dim_plant.sql                  # Plant dimension
â”‚   â”‚   â”‚   â”œâ”€â”€ dim_machine.sql                # Machine dimension
â”‚   â”‚   â”‚   â”œâ”€â”€ dim_test_equipment.sql         # Equipment dimension
â”‚   â”‚   â”‚   â”œâ”€â”€ dim_product.sql                # Product dimension
â”‚   â”‚   â”‚   â”œâ”€â”€ dim_date.sql                   # Date dimension
â”‚   â”‚   â”‚   â”œâ”€â”€ dim_shift.sql                  # Shift dimension
â”‚   â”‚   â”‚   â”œâ”€â”€ fact_manufacturing_test.sql    # Test results fact
â”‚   â”‚   â”‚   â”œâ”€â”€ fact_sensor_readings.sql       # Sensor readings fact
â”‚   â”‚   â”‚   â””â”€â”€ fact_production_output.sql     # Production fact
â”‚   â”‚   â”œâ”€â”€ docs/
â”‚   â”‚   â”‚   â”œâ”€â”€ dimensional-modeling-guide.md
â”‚   â”‚   â”‚   â”œâ”€â”€ grain-definition.md
â”‚   â”‚   â”‚   â””â”€â”€ conforming-dimensions.md
â”‚   â”‚   â””â”€â”€ diagrams/
â”‚   â”‚       â””â”€â”€ star-schema-erd.mmd            # Mermaid diagram
â”‚   â”‚
â”‚   â”œâ”€â”€ slowly-changing-dimensions/
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ scd_type1.py                   # Overwrite
â”‚   â”‚   â”‚   â”œâ”€â”€ scd_type2.py                   # Historical tracking
â”‚   â”‚   â”‚   â”œâ”€â”€ scd_type3.py                   # Limited history
â”‚   â”‚   â”‚   â”œâ”€â”€ scd_type6.py                   # Hybrid
â”‚   â”‚   â”‚   â””â”€â”€ dimension_builder.py           # Generic builder
â”‚   â”‚   â”œâ”€â”€ examples/
â”‚   â”‚   â”‚   â”œâ”€â”€ machine_config_scd2.py         # Your actual use case
â”‚   â”‚   â”‚   â”œâ”€â”€ plant_hierarchy_scd1.py
â”‚   â”‚   â”‚   â””â”€â”€ product_versioning_scd3.py
â”‚   â”‚   â””â”€â”€ docs/
â”‚   â”‚       â”œâ”€â”€ scd-type-comparison.md
â”‚   â”‚       â””â”€â”€ effective-dating-strategies.md
â”‚   â”‚
â”‚   â”œâ”€â”€ fact-table-patterns/
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”‚   â”œâ”€â”€ fact_builder.py
â”‚   â”‚   â”‚   â”œâ”€â”€ surrogate_key_generator.py
â”‚   â”‚   â”‚   â”œâ”€â”€ incremental_load.py
â”‚   â”‚   â”‚   â””â”€â”€ aggregate_fact_builder.py
â”‚   â”‚   â”œâ”€â”€ examples/
â”‚   â”‚   â”‚   â”œâ”€â”€ manufacturing_test_fact.py     # Billions of records
â”‚   â”‚   â”‚   â”œâ”€â”€ sensor_readings_fact.py
â”‚   â”‚   â”‚   â””â”€â”€ production_summary_fact.py
â”‚   â”‚   â””â”€â”€ docs/
â”‚   â”‚       â”œâ”€â”€ fact-table-types.md
â”‚   â”‚       â””â”€â”€ partitioning-strategies.md
â”‚   â”‚
â”‚   â”œâ”€â”€ multi-plant-hierarchy/
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”œâ”€â”€ hierarchical_dimensions.py         # Plant â†’ Site â†’ Division
â”‚   â”‚   â”œâ”€â”€ cross_plant_conformance.py
â”‚   â”‚   â””â”€â”€ rollup_aggregations.sql
â”‚   â”‚
â”‚   â””â”€â”€ incremental-loading/
â”‚       â”œâ”€â”€ README.md
â”‚       â”œâ”€â”€ cdc_merge.py
â”‚       â”œâ”€â”€ watermark_based_load.py
â”‚       â””â”€â”€ change_tracking.md
â”‚
â”œâ”€â”€ 05-performance-optimization/                 # PRIORITY 2
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ spark-ui-analysis/
â”‚   â”‚   â”œâ”€â”€ README.md                           # Your methodology
â”‚   â”‚   â”œâ”€â”€ guides/
â”‚   â”‚   â”‚   â”œâ”€â”€ reading-spark-ui.md            # How to read Spark UI
â”‚   â”‚   â”‚   â”œâ”€â”€ stage-analysis.md              # Analyze stages
â”‚   â”‚   â”‚   â”œâ”€â”€ identifying-bottlenecks.md     # Find slow operations
â”‚   â”‚   â”‚   â”œâ”€â”€ shuffle-analysis.md            # Understand shuffles
â”‚   â”‚   â”‚   â”œâ”€â”€ memory-analysis.md             # Memory issues
â”‚   â”‚   â”‚   â””â”€â”€ skew-detection.md              # Detect data skew
â”‚   â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ spark_ui_client.py            # REST API client
â”‚   â”‚   â”‚   â”œâ”€â”€ stage_analyzer.py             # Stage metrics
â”‚   â”‚   â”‚   â”œâ”€â”€ task_analyzer.py              # Task distribution
â”‚   â”‚   â”‚   â”œâ”€â”€ skew_detector.py              # Skew analysis
â”‚   â”‚   â”‚   â”œâ”€â”€ shuffle_analyzer.py           # Shuffle analysis
â”‚   â”‚   â”‚   â””â”€â”€ recommendation_engine.py      # Auto recommendations
â”‚   â”‚   â”œâ”€â”€ notebooks/
â”‚   â”‚   â”‚   â”œâ”€â”€ spark_ui_deep_dive.ipynb
â”‚   â”‚   â”‚   â””â”€â”€ performance_troubleshooting.ipynb
â”‚   â”‚   â””â”€â”€ examples/
â”‚   â”‚       â”œâ”€â”€ analyze_slow_job.py
â”‚   â”‚       â””â”€â”€ before_after_optimization.md
â”‚   â”‚
â”‚   â”œâ”€â”€ shuffle-optimization/
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”‚   â”œâ”€â”€ eliminating_shuffles.py        # Avoid shuffles
â”‚   â”‚   â”‚   â”œâ”€â”€ broadcast_joins.py             # Use broadcast
â”‚   â”‚   â”‚   â”œâ”€â”€ partition_optimization.py      # Partition tuning
â”‚   â”‚   â”‚   â””â”€â”€ coalesce_vs_repartition.py     # When to use each
â”‚   â”‚   â”œâ”€â”€ examples/
â”‚   â”‚   â”‚   â”œâ”€â”€ map_flattening_no_shuffle.py   # Your 11-step strategy
â”‚   â”‚   â”‚   â”œâ”€â”€ avoiding_explosions.py
â”‚   â”‚   â”‚   â””â”€â”€ salting_for_skew.py
â”‚   â”‚   â””â”€â”€ docs/
â”‚   â”‚       â”œâ”€â”€ shuffle-types.md
â”‚   â”‚       â””â”€â”€ optimization-patterns.md
â”‚   â”‚
â”‚   â”œâ”€â”€ billion-record-optimization/
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”‚   â”œâ”€â”€ partition_calculator.py        # Calculate optimal partitions
â”‚   â”‚   â”‚   â”œâ”€â”€ memory_tuner.py                # Executor/driver sizing
â”‚   â”‚   â”‚   â”œâ”€â”€ file_sizing_optimizer.py       # Optimize file sizes
â”‚   â”‚   â”‚   â””â”€â”€ z_order_optimizer.py           # Z-ordering strategy
â”‚   â”‚   â”œâ”€â”€ configs/
â”‚   â”‚   â”‚   â”œâ”€â”€ small_cluster.yaml             # <1TB
â”‚   â”‚   â”‚   â”œâ”€â”€ medium_cluster.yaml            # 1-10TB
â”‚   â”‚   â”‚   â””â”€â”€ large_cluster.yaml             # >10TB
â”‚   â”‚   â””â”€â”€ docs/
â”‚   â”‚       â”œâ”€â”€ scaling-guidelines.md
â”‚   â”‚       â””â”€â”€ cost-optimization.md
â”‚   â”‚
â”‚   â”œâ”€â”€ adaptive-query-execution/
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”œâ”€â”€ aqe-configuration.md
â”‚   â”‚   â”œâ”€â”€ skew-join-optimization.py
â”‚   â”‚   â””â”€â”€ dynamic-partition-coalescing.py
â”‚   â”‚
â”‚   â””â”€â”€ benchmarking/
â”‚       â”œâ”€â”€ README.md
â”‚       â”œâ”€â”€ benchmark_runner.py
â”‚       â”œâ”€â”€ tpcds_manufacturing_variant.sql
â”‚       â””â”€â”€ performance_regression_tests.py
â”‚
â”œâ”€â”€ 06-databricks-deployment/                    # PRIORITY 3
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ asset-bundles/
â”‚   â”‚   â”œâ”€â”€ README.md                           # Complete bundle guide
â”‚   â”‚   â”œâ”€â”€ databricks.yml                     # Main bundle config
â”‚   â”‚   â”œâ”€â”€ resources/
â”‚   â”‚   â”‚   â”œâ”€â”€ jobs/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ streaming_pipeline.yml
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ batch_processing.yml
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ dimensional_load.yml
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ monitoring_job.yml
â”‚   â”‚   â”‚   â”œâ”€â”€ clusters/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ streaming_cluster.yml
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ batch_cluster.yml
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ml_cluster.yml
â”‚   â”‚   â”‚   â”œâ”€â”€ workflows/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ end_to_end_etl.yml
â”‚   â”‚   â”‚   â””â”€â”€ pipelines/
â”‚   â”‚   â”‚       â””â”€â”€ delta_live_tables.yml
â”‚   â”‚   â”œâ”€â”€ environments/
â”‚   â”‚   â”‚   â”œâ”€â”€ dev.yaml
â”‚   â”‚   â”‚   â”œâ”€â”€ qa.yaml
â”‚   â”‚   â”‚   â””â”€â”€ prod.yaml
â”‚   â”‚   â””â”€â”€ docs/
â”‚   â”‚       â”œâ”€â”€ bundle-deployment-guide.md
â”‚   â”‚       â””â”€â”€ environment-promotion.md
â”‚   â”‚
â”‚   â”œâ”€â”€ hocon-configurations/
â”‚   â”‚   â”œâ”€â”€ README.md                           # Your HOCON patterns
â”‚   â”‚   â”œâ”€â”€ application.conf                   # Base config
â”‚   â”‚   â”œâ”€â”€ environments/
â”‚   â”‚   â”‚   â”œâ”€â”€ dev.conf
â”‚   â”‚   â”‚   â”œâ”€â”€ qa.conf
â”‚   â”‚   â”‚   â””â”€â”€ prod.conf
â”‚   â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”‚   â””â”€â”€ config_loader.py               # Load HOCON in Python
â”‚   â”‚   â””â”€â”€ docs/
â”‚   â”‚       â””â”€â”€ hocon-best-practices.md
â”‚   â”‚
â”‚   â”œâ”€â”€ github-actions-cicd/
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”œâ”€â”€ workflows/
â”‚   â”‚   â”‚   â”œâ”€â”€ databricks-deploy.yml          # Main deployment
â”‚   â”‚   â”‚   â”œâ”€â”€ bundle-validate.yml            # PR validation
â”‚   â”‚   â”‚   â”œâ”€â”€ integration-tests.yml          # Run tests
â”‚   â”‚   â”‚   â””â”€â”€ smoke-tests.yml                # Post-deploy tests
â”‚   â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â”‚   â”œâ”€â”€ deploy_to_workspace.sh
â”‚   â”‚   â”‚   â”œâ”€â”€ run_integration_tests.sh
â”‚   â”‚   â”‚   â””â”€â”€ rollback_deployment.sh
â”‚   â”‚   â””â”€â”€ docs/
â”‚   â”‚       â”œâ”€â”€ cicd-pipeline-design.md
â”‚   â”‚       â””â”€â”€ deployment-strategies.md
â”‚   â”‚
â”‚   â”œâ”€â”€ cluster-management/
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”œâ”€â”€ autoscaling-strategies.md
â”‚   â”‚   â”œâ”€â”€ instance-pool-setup.md
â”‚   â”‚   â””â”€â”€ cost-optimization.md
â”‚   â”‚
â”‚   â””â”€â”€ workspace-organization/
â”‚       â”œâ”€â”€ folder-structure.md
â”‚       â”œâ”€â”€ naming-conventions.md
â”‚       â””â”€â”€ access-control-patterns.md
â”‚
â”œâ”€â”€ 07-enterprise-integration/                   # PRIORITY 3
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ sap-integration/
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”œâ”€â”€ extractors/
â”‚   â”‚   â”‚   â”œâ”€â”€ sap_bapi_extractor.py
â”‚   â”‚   â”‚   â”œâ”€â”€ sap_table_extractor.py
â”‚   â”‚   â”‚   â””â”€â”€ sap_rfc_connector.py
â”‚   â”‚   â”œâ”€â”€ change-data-capture/
â”‚   â”‚   â”‚   â”œâ”€â”€ cdc_pattern.md
â”‚   â”‚   â”‚   â””â”€â”€ incremental_extraction.py
â”‚   â”‚   â””â”€â”€ docs/
â”‚   â”‚       â””â”€â”€ sap-integration-architecture.md
â”‚   â”‚
â”‚   â”œâ”€â”€ opp-data-processing/                    # Your OPP system
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”‚   â”œâ”€â”€ opp_data_ingestion.py
â”‚   â”‚   â”‚   â””â”€â”€ opp_transformation.py
â”‚   â”‚   â””â”€â”€ docs/
â”‚   â”‚       â””â”€â”€ opp-integration-guide.md
â”‚   â”‚
â”‚   â”œâ”€â”€ change-data-capture/
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”œâ”€â”€ debezium-kafka-delta/
â”‚   â”‚   â”‚   â”œâ”€â”€ debezium-connector-config.json
â”‚   â”‚   â”‚   â””â”€â”€ cdc_consumer.py
â”‚   â”‚   â””â”€â”€ incremental-processing/
â”‚   â”‚       â”œâ”€â”€ watermark_strategy.py
â”‚   â”‚       â””â”€â”€ merge_upsert.py
â”‚   â”‚
â”‚   â””â”€â”€ orchestration/
â”‚       â”œâ”€â”€ airflow-dags/
â”‚       â”‚   â”œâ”€â”€ manufacturing_etl_dag.py
â”‚       â”‚   â””â”€â”€ dimensional_load_dag.py
â”‚       â””â”€â”€ dependency-management/
â”‚           â””â”€â”€ task-dependency-patterns.md
â”‚
â”œâ”€â”€ 08-testing-validation/                       # PRIORITY 3
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ vasapi-validator/                       # Your test framework
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”œâ”€â”€ integration/
â”‚   â”‚   â”‚   â”œâ”€â”€ vasapi_setup.md
â”‚   â”‚   â”‚   â””â”€â”€ test_runner.py
â”‚   â”‚   â””â”€â”€ examples/
â”‚   â”‚       â”œâ”€â”€ data_quality_tests.py
â”‚   â”‚       â””â”€â”€ pipeline_validation.py
â”‚   â”‚
â”‚   â”œâ”€â”€ data-quality-testing/
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”‚   â”œâ”€â”€ quality_checks.py
â”‚   â”‚   â”‚   â”œâ”€â”€ schema_validation.py
â”‚   â”‚   â”‚   â”œâ”€â”€ completeness_checks.py
â”‚   â”‚   â”‚   â””â”€â”€ accuracy_checks.py
â”‚   â”‚   â””â”€â”€ great-expectations/
â”‚   â”‚       â””â”€â”€ expectations/
â”‚   â”‚
â”‚   â”œâ”€â”€ integration-testing/
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”œâ”€â”€ test-containers/
â”‚   â”‚   â”‚   â””â”€â”€ docker-compose.test.yml
â”‚   â”‚   â””â”€â”€ tests/
â”‚   â”‚       â”œâ”€â”€ test_kafka_to_delta.py
â”‚   â”‚       â””â”€â”€ test_dimensional_load.py
â”‚   â”‚
â”‚   â””â”€â”€ performance-testing/
â”‚       â”œâ”€â”€ README.md
â”‚       â”œâ”€â”€ load_tests/
â”‚       â””â”€â”€ regression_tests/
â”‚
â”œâ”€â”€ 09-monitoring-observability/                 # PRIORITY 3
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ prometheus-monitoring/
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”œâ”€â”€ metrics/
â”‚   â”‚   â”‚   â”œâ”€â”€ pipeline_metrics.py
â”‚   â”‚   â”‚   â”œâ”€â”€ kafka_metrics.py
â”‚   â”‚   â”‚   â””â”€â”€ delta_metrics.py
â”‚   â”‚   â”œâ”€â”€ prometheus.yml
â”‚   â”‚   â””â”€â”€ alert-rules.yml
â”‚   â”‚
â”‚   â”œâ”€â”€ grafana-dashboards/
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”œâ”€â”€ dashboards/
â”‚   â”‚   â”‚   â”œâ”€â”€ streaming-pipeline.json
â”‚   â”‚   â”‚   â”œâ”€â”€ kafka-cluster.json
â”‚   â”‚   â”‚   â”œâ”€â”€ spark-performance.json
â”‚   â”‚   â”‚   â””â”€â”€ data-quality.json
â”‚   â”‚   â””â”€â”€ provisioning/
â”‚   â”‚       â””â”€â”€ datasources.yml
â”‚   â”‚
â”‚   â”œâ”€â”€ logging/
â”‚   â”‚   â”œâ”€â”€ structured-logging.py
â”‚   â”‚   â”œâ”€â”€ log-aggregation.md
â”‚   â”‚   â””â”€â”€ log-analysis-queries.md
â”‚   â”‚
â”‚   â””â”€â”€ alerting/
â”‚       â”œâ”€â”€ pagerduty-integration.md
â”‚       â”œâ”€â”€ slack-notifications.py
â”‚       â””â”€â”€ alert-runbooks/
â”‚
â”œâ”€â”€ 10-production-playbooks/                     # PRIORITY 1
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ troubleshooting/
â”‚   â”‚   â”œâ”€â”€ schema-evolution-errors.md         # Your daily issues
â”‚   â”‚   â”œâ”€â”€ kafka-auth-failures.md
â”‚   â”‚   â”œâ”€â”€ delta-merge-conflicts.md
â”‚   â”‚   â”œâ”€â”€ checkpoint-recovery.md
â”‚   â”‚   â”œâ”€â”€ performance-degradation.md
â”‚   â”‚   â”œâ”€â”€ data-skew-issues.md
â”‚   â”‚   â””â”€â”€ out-of-memory-errors.md
â”‚   â”‚
â”‚   â”œâ”€â”€ runbooks/
â”‚   â”‚   â”œâ”€â”€ pipeline-restart.md
â”‚   â”‚   â”œâ”€â”€ schema-migration.md
â”‚   â”‚   â”œâ”€â”€ cluster-scaling.md
â”‚   â”‚   â”œâ”€â”€ incident-response.md
â”‚   â”‚   â””â”€â”€ disaster-recovery.md
â”‚   â”‚
â”‚   â”œâ”€â”€ operational-procedures/
â”‚   â”‚   â”œâ”€â”€ deployment-checklist.md
â”‚   â”‚   â”œâ”€â”€ rollback-procedure.md
â”‚   â”‚   â”œâ”€â”€ data-backfill.md
â”‚   â”‚   â””â”€â”€ maintenance-windows.md
â”‚   â”‚
â”‚   â””â”€â”€ error-reference/
â”‚       â”œâ”€â”€ kafka-errors.md
â”‚       â”œâ”€â”€ spark-errors.md
â”‚       â”œâ”€â”€ delta-errors.md
â”‚       â””â”€â”€ databricks-errors.md
â”‚
â”œâ”€â”€ 11-real-world-case-studies/                 # YOUR PROJECTS
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ multi-plant-streaming/
â”‚   â”‚   â”œâ”€â”€ README.md                          # Document your architecture
â”‚   â”‚   â”œâ”€â”€ architecture-diagram.mmd
â”‚   â”‚   â”œâ”€â”€ data-flow.md
â”‚   â”‚   â”œâ”€â”€ challenges-solved.md
â”‚   â”‚   â””â”€â”€ lessons-learned.md
â”‚   â”‚
â”‚   â”œâ”€â”€ billion-record-processing/
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”œâ”€â”€ optimization-journey.md            # Your optimizations
â”‚   â”‚   â”œâ”€â”€ performance-metrics.md
â”‚   â”‚   â””â”€â”€ cost-analysis.md
â”‚   â”‚
â”‚   â”œâ”€â”€ opp-data-integration/
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”œâ”€â”€ integration-architecture.md
â”‚   â”‚   â””â”€â”€ data-pipeline.md
â”‚   â”‚
â”‚   â””â”€â”€ sap-manufacturing-integration/
â”‚       â”œâ”€â”€ README.md
â”‚       â”œâ”€â”€ extraction-patterns.md
â”‚       â””â”€â”€ transformation-logic.md
â”‚
â”œâ”€â”€ docs/                                        # General documentation
â”‚   â”œâ”€â”€ architecture/
â”‚   â”‚   â”œâ”€â”€ platform-overview.md
â”‚   â”‚   â”œâ”€â”€ technology-decisions.md
â”‚   â”‚   â””â”€â”€ patterns-catalog.md
â”‚   â”œâ”€â”€ best-practices/
â”‚   â”‚   â”œâ”€â”€ coding-standards.md
â”‚   â”‚   â”œâ”€â”€ naming-conventions.md
â”‚   â”‚   â””â”€â”€ code-review-checklist.md
â”‚   â””â”€â”€ reference/
â”‚       â”œâ”€â”€ technology-stack.md
â”‚       â”œâ”€â”€ glossary.md
â”‚       â””â”€â”€ useful-resources.md
â”‚
â”œâ”€â”€ tools/                                       # Utility tools
â”‚   â”œâ”€â”€ config-generators/
â”‚   â”‚   â”œâ”€â”€ kafka-config-gen.py
â”‚   â”‚   â”œâ”€â”€ spark-config-gen.py
â”‚   â”‚   â””â”€â”€ databricks-bundle-gen.py
â”‚   â”œâ”€â”€ data-generators/
â”‚   â”‚   â”œâ”€â”€ sensor_data_generator.py
â”‚   â”‚   â””â”€â”€ test_data_factory.py
â”‚   â””â”€â”€ scripts/
â”‚       â”œâ”€â”€ setup-dev-environment.sh
â”‚       â”œâ”€â”€ validate-configs.sh
â”‚       â””â”€â”€ generate-documentation.sh
â”‚
â””â”€â”€ examples/                                    # Complete examples
    â”œâ”€â”€ quickstart/
    â”‚   â”œâ”€â”€ 01_kafka_to_delta_basic.py
    â”‚   â”œâ”€â”€ 02_dimensional_modeling.py
    â”‚   â””â”€â”€ 03_spark_optimization.py
    â””â”€â”€ end-to-end/
        â””â”€â”€ manufacturing_etl_pipeline/
            â”œâ”€â”€ README.md
            â”œâ”€â”€ configs/
            â”œâ”€â”€ src/
            â”œâ”€â”€ tests/
            â””â”€â”€ deployment/
```

---

## ğŸ¯ PHASE 1: FOUNDATION (Week 1-2)

### Task 1: Repository Setup & Documentation

**Create base files:**

1. **README.md** (Repository root):
```markdown
# Manufacturing Data Platform - Production Patterns

> Enterprise-scale data engineering patterns for manufacturing IoT, tested in production with billions of records across multi-plant deployments.

## ğŸ¯ What This Repository Provides

This repository documents **battle-tested patterns** for:
- **Real-time streaming**: Kafka â†’ Spark â†’ Delta Lake at scale
- **Schema evolution**: Handling type conflicts, MAP flattening, production errors
- **Performance optimization**: Spark UI analysis, billion-record optimization
- **Dimensional modeling**: Manufacturing star schemas, SCD patterns
- **Enterprise integration**: SAP, OPP, multi-system orchestration
- **Production operations**: Troubleshooting playbooks, runbooks, deployment

## ğŸ­ Built for Manufacturing

These patterns solve real challenges in:
- Multi-plant IoT sensor data processing
- Manufacturing test equipment data (billions of records)
- Production floor real-time analytics
- Quality control and compliance tracking
- Cross-plant data integration and conformance

## âš¡ Quick Start

### Prerequisites
- Apache Kafka 3.x with SASL_SSL
- Apache Spark 3.4+ / Databricks Runtime 13.x+
- Delta Lake 3.0+
- Python 3.10+
- Java 11+ (for Kafka Streams)

### 1. Kafka SASL_SSL Authentication
```python
from manufacturing_platform.auth import get_kafka_sasl_config

# Get production Kafka config from Databricks secrets
kafka_config = get_kafka_sasl_config(env="prod")

# Use in streaming pipeline
df = spark.readStream.format("kafka") \
    .options(**kafka_config) \
    .option("subscribe", "sensor-events") \
    .load()
```

See: [Kafka SASL_SSL Guide](01-authentication-security/kafka-sasl-ssl/README.md)

### 2. Handle Schema Evolution
```python
from manufacturing_platform.schema import TypeConflictResolver

# Resolve DELTA_MERGE_INCOMPATIBLE_DATATYPE errors
resolver = TypeConflictResolver()
conflicts = resolver.identify_conflicts(source_df.schema, target_schema)
resolved_df = resolver.resolve_conflicts(source_df, conflicts)
```

See: [Schema Evolution Guide](02-schema-evolution/README.md)

### 3. Optimize for Billions of Records
```python
from manufacturing_platform.optimization import optimize_for_scale

# Auto-configure Spark for your data volume
spark = optimize_for_scale(
    spark,
    data_volume_gb=5000,
    partition_keys=["plant_id", "date"]
)
```

See: [Performance Optimization Guide](05-performance-optimization/README.md)

## ğŸ“š Pattern Catalog

### Production-Critical Patterns
1. [Kafka SASL_SSL Authentication](01-authentication-security/kafka-sasl-ssl/) - Troubleshoot auth failures
2. [Delta Type Conflicts](02-schema-evolution/delta-type-conflicts/) - Handle merge errors
3. [MAP Type Flattening](02-schema-evolution/map-type-flattening/) - 11-step strategy
4. [Spark UI Analysis](05-performance-optimization/spark-ui-analysis/) - Performance methodology

### Architecture Patterns
5. [Kafka Streams + Spark](03-streaming-architecture/kstreams-preprocessing/) - Hybrid architecture
6. [Dimensional Modeling](04-dimensional-modeling/star-schema-design/) - Manufacturing DWH
7. [SCD Type 2](04-dimensional-modeling/slowly-changing-dimensions/) - Historical tracking

### Deployment Patterns
8. [Databricks Bundles](06-databricks-deployment/asset-bundles/) - Multi-env deployment
9. [GitHub Actions CI/CD](06-databricks-deployment/github-actions-cicd/) - Automated deployment

### Operational Patterns
10. [Production Playbooks](10-production-playbooks/) - Troubleshooting guides

## ğŸ† Real-World Impact

These patterns power production systems processing:
- **5M+ events/day** from manufacturing sensors
- **Billions of records** in Delta Lake tables
- **Sub-30 second latency** for real-time analytics
- **99.9% uptime** across multiple plants
- **40% cost reduction** through optimization

## ğŸ¤ Contributing

This repository is built from real production experience. Contributions welcome for:
- New patterns from your production deployments
- Troubleshooting guides for errors you've solved
- Performance optimization techniques
- Architecture decision records

See [CONTRIBUTING.md](CONTRIBUTING.md)

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE)

---

**Built with â¤ï¸ for manufacturing data engineers**
```

2. **ARCHITECTURE.md**:
```markdown
# Platform Architecture

## Design Principles

### 1. Production-First
All patterns in this repository are derived from production deployments, not theoretical examples.

### 2. Multi-Plant Scale
Designed for enterprise manufacturing with:
- Multiple plants across geographies
- Diverse equipment and sensor types
- Billions of records
- Complex organizational hierarchies

### 3. Real-Time + Batch Hybrid
Combines streaming for real-time insights with batch for complex analytics:
- **Kafka Streams**: Sub-second preprocessing
- **Spark Streaming**: Medallion architecture (Bronze â†’ Silver â†’ Gold)
- **Batch Processing**: Dimensional modeling, aggregations

### 4. Schema Flexibility
Manufacturing environments have unpredictable schema evolution:
- Sensors added/removed dynamically
- Firmware updates change data structures
- Cross-plant schema variations

## Technology Stack

### Core Technologies
- **Messaging**: Apache Kafka 3.6+ (SASL_SSL secured)
- **Stream Processing**: Kafka Streams 3.6+, Spark Structured Streaming 3.4+
- **Storage**: Delta Lake 3.0+ on cloud object storage
- **Compute**: Databricks Runtime 13.x+ (or open-source Spark)
- **Data Warehouse**: Dimensional modeling with Delta Lake

### Integration Layer
- **SAP**: BAPI/RFC extractors for master data
- **OPP**: Offline Programming Platform integration
- **MES**: Manufacturing Execution Systems
- **Quality Systems**: SPC, 6-sigma tools

### DevOps
- **Deployment**: Databricks Asset Bundles
- **CI/CD**: GitHub Actions
- **Configuration**: HOCON for type-safe configs
- **Monitoring**: Prometheus + Grafana
- **Testing**: VasapiValidator framework

## Architecture Decisions

### ADR-001: Kafka Streams for Preprocessing
**Context**: Need low-latency stateful operations before Spark

**Decision**: Use Kafka Streams for:
- Data validation (malformed JSON rejection)
- Machine metadata enrichment
- Windowed aggregations (1-minute windows)
- Topic compaction

**Rationale**:
- Sub-second latency
- Stateful operations without external state store
- Natural fit for per-record processing

**Consequences**:
- Need Java codebase in addition to Python
- Adds operational complexity (deploy KStreams apps)
- Better separation of concerns

### ADR-002: Delta Lake for All Storage
**Context**: Need ACID, time travel, schema evolution

**Decision**: Use Delta Lake for Bronze/Silver/Gold layers

**Rationale**:
- ACID transactions
- Schema evolution with `mergeSchema`
- Time travel for debugging and compliance
- Efficient upserts with MERGE
- Z-ordering for query performance

**Consequences**:
- Vendor lock-in considerations (open-source Delta protocol)
- Requires understanding of Delta internals
- Optimized for lakehouse, not data warehouse

### ADR-003: Medallion Architecture
**Context**: Need data quality tiers

**Decision**: Implement Bronze â†’ Silver â†’ Gold

**Bronze**: Raw ingestion, preserve everything
**Silver**: Cleaned, validated, conformed
**Gold**: Business-level aggregations

**Rationale**:
- Clear data quality contracts
- Enables reprocessing from raw
- Separation of concerns

### ADR-004: Hybrid KStreams + Spark
**Context**: Need both low-latency and complex transformations

**Decision**: KStreams preprocessing â†’ Spark complex SQL

**KStreams for**:
- Validation
- Enrichment
- Simple windowing

**Spark for**:
- Complex SQL transformations
- Joins with large dimensions
- ML model inference
- Delta Lake writes

[Continue with more ADRs...]
```

3. **CONTRIBUTING.md**:
```markdown
# Contributing to Manufacturing Data Platform

## Contribution Philosophy

This repository captures **real production patterns**, not theoretical examples. When contributing:

1. **Production-Tested**: Pattern should be deployed in production
2. **Problem-Solving**: Document the problem solved, not just the solution
3. **Scale-Proven**: If claiming performance benefits, include metrics
4. **Troubleshooting-First**: Include error scenarios and solutions

## Types of Contributions

### 1. Production Patterns
Share a pattern from your deployment:
- **Problem**: What production issue did this solve?
- **Solution**: Implementation details
- **Metrics**: Performance impact (before/after)
- **Lessons Learned**: What would you do differently?

### 2. Troubleshooting Guides
Document an error you solved:
- **Error Message**: Exact error text
- **Root Cause**: Why it occurred
- **Solution**: Step-by-step fix
- **Prevention**: How to avoid in future

### 3. Performance Optimizations
Share optimization techniques:
- **Baseline**: Performance before optimization
- **Changes**: What you changed and why
- **Results**: Performance after optimization
- **Trade-offs**: What you sacrificed (if anything)

### 4. Architecture Decisions
Document important decisions:
- **Context**: What problem needed solving?
- **Options Considered**: What alternatives did you evaluate?
- **Decision**: What did you choose and why?
- **Consequences**: What are the implications?

## Contribution Process

### 1. Check Existing Patterns
Search the repository to avoid duplicates.

### 2. Open an Issue First
Describe what you plan to contribute. Get feedback before writing code.

### 3. Follow Structure
Use the appropriate template for your contribution type.

### 4. Include Complete Examples
- Working code (tested)
- Configuration files
- Test data (synthetic, no real data)
- Documentation

### 5. Submit Pull Request
- Reference the issue
- Describe changes
- Include test results

## Code Standards

### Python
- PEP 8 compliant
- Type hints (Python 3.10+)
- Google-style docstrings
- 100 character line length

### Java (Kafka Streams)
- Java 11+
- Google Java Style Guide
- Javadoc for public APIs

### SQL
- Uppercase keywords
- Lowercase table/column names
- Indented for readability

### Documentation
- Markdown for all docs
- Mermaid for diagrams
- Code examples with comments

## Testing Requirements

### Unit Tests
- Required for all code contributions
- pytest for Python
- JUnit for Java

### Integration Tests
- Required for architectural patterns
- Use test containers

### Performance Tests
- Required for optimization claims
- Include benchmark scripts
- Document test environment

## Review Process

1. **Automated Checks**: CI/CD pipeline runs
2. **Peer Review**: At least one maintainer reviews
3. **Production Validation**: Confirm pattern is production-tested
4. **Documentation Review**: Ensure docs are clear and complete

## Recognition

Contributors will be:
- Listed in README
- Mentioned in release notes
- Featured in pattern documentation

## Questions?

Open a GitHub Discussion or email the maintainers.

---

Thank you for contributing! ğŸ™
```

---

### Task 2: Kafka SASL_SSL Authentication (CRITICAL)

**Location**: `01-authentication-security/kafka-sasl-ssl/`

**Files to create** (see Option A prompt for detailed code - same implementation)

---

### Task 3: Schema Evolution & Type Conflicts (CRITICAL)

**Location**: `02-schema-evolution/delta-type-conflicts/`

**Files to create** (see Option A prompt for detailed code)

---

### Task 4: MAP Type Flattening (YOUR SPECIALTY)

**Location**: `02-schema-evolution/map-type-flattening/`

**Create**: `docs/11-step-methodology.md`

```markdown
# 11-Step Progressive Flattening Methodology

## Problem Statement

Manufacturing IoT sensors send data with `customExtension` MAP<STRING, STRING> fields containing:
- Unpredictable keys (different per machine, firmware version)
- 100+ potential fields
- Nested structures (sometimes JSON strings as values)
- Frequent schema changes without notice

**Naive Approach (Don't Do This)**:
```python
# âŒ Causes expensive shuffle, OOM on large data
df.select("*", explode(col("customExtension")))
```

## The 11-Step Strategy

### Step 1: Sample-Based Key Discovery
Discover keys without full table scan:
```python
def discover_keys(df: DataFrame, sample_size: int = 1000) -> List[str]:
    """
    Sample-based key discovery.
    Cost: O(sample_size) instead of O(n)
    """
    keys_df = (
        df.select(explode(map_keys(col("customExtension"))).alias("key"))
        .limit(sample_size)
        .distinct()
    )
    return [row.key for row in keys_df.collect()]
```

### Step 2: Extract Known Keys Without Explosion
Use getItem() instead of explode():
```python
known_keys = ["temperature", "pressure", "vibration", "rpm"]

for key in known_keys:
    df = df.withColumn(
        f"ext_{key}",
        col("customExtension").getItem(key)
    )
# âœ… No shuffle! Column-wise operation
```

### Step 3: Handle Null/Missing Keys
```python
# Provide defaults for missing keys
df = df.withColumn(
    "ext_temperature",
    coalesce(col("customExtension").getItem("temperature"), lit(None))
)
```

### Step 4: Type Inference
```python
def infer_and_cast(df: DataFrame, key: str) -> DataFrame:
    """
    Infer type from sample and cast.
    """
    # Sample values
    sample_values = df.select(col("customExtension").getItem(key)).limit(100).collect()

    # Infer type (numeric, timestamp, string, json)
    inferred_type = infer_type_from_samples(sample_values)

    # Cast accordingly
    if inferred_type == "numeric":
        df = df.withColumn(f"ext_{key}", col("customExtension").getItem(key).cast("double"))
    elif inferred_type == "timestamp":
        df = df.withColumn(f"ext_{key}", to_timestamp(col("customExtension").getItem(key)))
    # ... etc

    return df
```

### Step 5: Nested JSON Handling
```python
# Some values are JSON strings - parse them
df = df.withColumn(
    "ext_config_parsed",
    when(
        col("customExtension").getItem("config").isNotNull(),
        from_json(col("customExtension").getItem("config"), config_schema)
    ).otherwise(lit(None))
)
```

### Step 6: Explosion Only When Necessary
```python
# Only explode for truly dynamic keys (rare case)
# AND only after filtering to small subset
filtered_df = df.filter("plant_id = 'PLANT_001' AND date = '2025-01-01'")  # Small subset
exploded_df = filtered_df.select("*", explode(col("customExtension")))
```

### Step 7: Partition Before Explosion
```python
# If explosion is unavoidable, partition first to parallelize
df = df.repartition(200, "machine_id")  # Spread across executors
exploded_df = df.select("*", explode(col("customExtension")))
```

### Step 8: Incremental Key Addition
```python
# Track new keys over time, add incrementally
current_keys = get_current_extracted_keys()  # From metadata table
discovered_keys = discover_keys(df)

new_keys = set(discovered_keys) - set(current_keys)

if new_keys:
    logger.info(f"Found {len(new_keys)} new keys: {new_keys}")
    for key in new_keys:
        df = df.withColumn(f"ext_{key}", col("customExtension").getItem(key))
        register_new_key(key)  # Update metadata
```

### Step 9: Schema Evolution-Friendly Storage
```python
# Write with schema evolution enabled
df.write.format("delta") \
    .mode("append") \
    .option("mergeSchema", "true") \  # Allow new columns
    .save(path)
```

### Step 10: Optimize File Layout
```python
# After adding many columns, optimize
spark.sql(f"""
    OPTIMIZE delta.`{path}`
    ZORDER BY (machine_id, timestamp)
""")
```

### Step 11: Monitor Column Count
```python
# Alert if too many columns
num_columns = len(df.columns)
if num_columns > 1000:
    logger.warning(f"Table has {num_columns} columns. Consider archiving old columns.")
```

## Performance Comparison

| Approach | Duration | Shuffle GB | Memory |
|----------|----------|------------|--------|
| Naive explosion | 45 min | 500 GB | OOM |
| 11-step strategy | 4 min | 5 GB | Stable |

## When to Use

âœ… **Use this pattern when**:
- MAP columns with 10+ dynamic keys
- Frequent schema changes
- Large data volumes (>1TB)
- Mixed data types in values

âŒ **Don't use when**:
- Fixed, known keys (just extract directly)
- Small data (<100GB)
- Values are uniform type

## Real-World Example

See: [Manufacturing IoT Sensors](../examples/iot_sensor_flattening.py)
```

---

## ğŸ¯ PHASE 2: CORE PATTERNS (Week 3-4)

Continue with:
- Dimensional modeling
- Kafka Streams + Spark integration
- Spark UI analysis methodology
- Databricks deployment

(Use Option A code examples - same implementations)

---

## ğŸ¯ PHASE 3: PRODUCTION OPERATIONS (Week 5-6)

### Production Playbooks

**Location**: `10-production-playbooks/troubleshooting/`

**Create**: `schema-evolution-errors.md`

```markdown
# Schema Evolution Errors - Production Playbook

## Error: DELTA_MERGE_INCOMPATIBLE_DATATYPE

### Symptom
```
org.apache.spark.sql.delta.schema.DeltaInvariantViolationException:
DELTA_MERGE_INCOMPATIBLE_DATATYPE: Cannot merge field 'customExtension'
of type MAP<STRING, STRING> with MAP<STRING, STRUCT<...>>
```

### When It Occurs
- Source schema changed (firmware update, new sensor type)
- Attempting MERGE operation with incompatible types
- Usually happens in Silver/Gold layer writes

### Root Cause
Delta Lake strict schema enforcement. MAP key types must match, value types must be compatible.

### Immediate Fix

**Option 1: Cast to Compatible Type** (Recommended)
```python
from pyspark.sql.functions import col

# Identify the conflict
source_type = source_df.schema["customExtension"].dataType
target_type = target_df.schema["customExtension"].dataType

# Cast source to match target (or vice versa)
source_df = source_df.withColumn(
    "customExtension",
    col("customExtension").cast(target_type)
)

# Now merge works
target_table.merge(source_df, merge_condition) \
    .whenMatchedUpdateAll() \
    .whenNotMatchedInsertAll() \
    .execute()
```

**Option 2: Column Versioning**
```python
# Keep both versions
source_df = source_df.withColumnRenamed("customExtension", "customExtension_v2")

# Write without merge (append mode)
source_df.write.format("delta").mode("append").save(path)

# Update downstream to use v2 when available
```

**Option 3: Rewrite Target Schema** (Use with caution)
```python
# âš ï¸  This rewrites the entire table!
source_df.write.format("delta") \
    .mode("overwrite") \
    .option("overwriteSchema", "true") \
    .save(path)
```

### Long-Term Prevention

1. **Schema Validation Gate**
```python
# Add to pipeline BEFORE merge
from manufacturing_platform.schema import validate_schema_compatibility

is_compatible = validate_schema_compatibility(source_df.schema, target_schema)

if not is_compatible:
    # Send to DLQ or alert
    raise SchemaIncompatibilityError("Schema validation failed")
```

2. **Automated Type Promotion**
```python
# Use type conflict resolver
from manufacturing_platform.schema import TypeConflictResolver

resolver = TypeConflictResolver()
resolved_df = resolver.auto_resolve(source_df, target_schema)
```

3. **Schema Registry Integration**
```python
# Validate against schema registry before write
from confluent_kafka.schema_registry import SchemaRegistryClient

sr_client = SchemaRegistryClient({"url": "http://schema-registry:8081"})
# Validate schema before merge
```

### Monitoring & Alerting

Add this metric to Prometheus:
```python
schema_conflicts_total.labels(table="manufacturing_silver").inc()
```

Alert if > 10 conflicts/hour:
```yaml
# prometheus/alerts.yml
- alert: HighSchemaConflicts
  expr: rate(schema_conflicts_total[1h]) > 10
  annotations:
    summary: "High schema conflict rate on {{ $labels.table }}"
```

### Post-Incident

1. **Root Cause Analysis**
   - Which system produced incompatible schema?
   - Was it a planned firmware update?
   - Communication breakdown?

2. **Update Runbook**
   - Add this specific error pattern
   - Document resolution

3. **Improve Prevention**
   - Coordinate with firmware teams
   - Add schema validation earlier in pipeline

### Related Errors
- `DELTA_SCHEMA_MISMATCH`
- `DELTA_INCOMPATIBLE_SCHEMA_MERGE`
- `DELTA_MERGE_PRECONDITION_VIOLATED`

### See Also
- [Type Conflict Resolver](../../02-schema-evolution/delta-type-conflicts/src/type_conflict_resolver.py)
- [Schema Validation Guide](../../02-schema-evolution/README.md)
```

**Create**: `kafka-auth-failures.md`

```markdown
# Kafka Authentication Failures - Production Playbook

## Error: "Authentication failed: Invalid username or password"

### Symptom
```
org.apache.kafka.common.errors.SaslAuthenticationException:
Authentication failed: Invalid username or password
```

### Diagnostic Steps

#### Step 1: Verify Credentials in Secrets
```python
# In Databricks notebook
secret_scope = "kafka-prod"

# This should return [REDACTED], not error
try:
    username = dbutils.secrets.get(secret_scope, "username")
    print("Username secret exists: âœ“")
except Exception as e:
    print(f"âŒ Username secret missing: {e}")

try:
    password = dbutils.secrets.get(secret_scope, "password")
    print("Password secret exists: âœ“")
except Exception as e:
    print(f"âŒ Password secret missing: {e}")
```

#### Step 2: Test Basic Network Connectivity
```bash
# From Databricks cluster driver node
%sh
telnet <kafka-broker-host> 9093
# Should connect (Ctrl+C to exit)
```

#### Step 3: Verify SASL Mechanism
```python
# Check what mechanism broker expects vs what you're using
expected_mechanism = "SCRAM-SHA-512"  # Check with Kafka admin
your_mechanism = kafka_config.get("kafka.sasl.mechanism")

if expected_mechanism != your_mechanism:
    print(f"âŒ Mechanism mismatch! Expected: {expected_mechanism}, Using: {your_mechanism}")
```

#### Step 4: Test with kafkacat (Debugging Tool)
```bash
# Install kafkacat on driver
%sh
apt-get install -y kafkacat

# Test connection
kafkacat -b <broker>:9093 \
  -X security.protocol=SASL_SSL \
  -X sasl.mechanism=PLAIN \
  -X sasl.username=<username> \
  -X sasl.password=<password> \
  -L  # List metadata

# If this works, issue is in Spark config
# If this fails, issue is with credentials/network
```

### Common Causes & Fixes

#### Cause 1: Expired Credentials
**Fix**: Rotate secrets in Azure Key Vault / AWS Secrets Manager
```python
# Update Databricks secret from Key Vault
# (Usually done via Databricks CLI or Terraform)
```

#### Cause 2: Wrong SASL Mechanism
**Fix**: Match broker configuration
```python
# Change from PLAIN to SCRAM-SHA-512
kafka_config = {
    "kafka.sasl.mechanism": "SCRAM-SHA-512",  # Was: "PLAIN"
    # ... rest of config
}
```

#### Cause 3: Incorrect JAAS Config Format
**Fix**: Validate JAAS string
```python
# âŒ Wrong (missing semicolon, wrong quotes)
jaas_config = """org.apache.kafka.common.security.plain.PlainLoginModule required
username='user' password='pass'"""

# âœ… Correct
jaas_config = """org.apache.kafka.common.security.plain.PlainLoginModule required
username="user"
password="pass";"""
```

#### Cause 4: Secret Scope Permissions
**Fix**: Grant cluster access to secret scope
```bash
# Databricks CLI
databricks secrets put-acl \
  --scope kafka-prod \
  --principal <cluster-service-principal> \
  --permission READ
```

### Quick Fix Script

```python
from manufacturing_platform.auth import diagnose_kafka_auth

# Run diagnostics
diagnosis = diagnose_kafka_auth(
    broker="kafka-prod.company.com:9093",
    secret_scope="kafka-prod",
    mechanism="SCRAM-SHA-512"
)

# Prints:
# âœ“ Secret scope accessible
# âœ“ Network connectivity OK
# âœ“ SASL mechanism matches broker
# âœ“ Credentials valid
# OR
# âŒ Issue found: [specific problem]
```

### Prevention

1. **Automated Secret Rotation**
   - Set up Key Vault rotation policy
   - Test credentials before expiry

2. **Connection Validation in CI/CD**
   ```yaml
   # .github/workflows/validate-kafka-auth.yml
   - name: Validate Kafka Connection
     run: |
       python scripts/test_kafka_connectivity.py --env prod
   ```

3. **Monitoring**
   ```python
   # Add metric
   kafka_auth_failures_total.labels(env="prod").inc()
   ```

### Escalation

If diagnostics don't resolve:
1. Contact Kafka platform team
2. Provide:
   - Exact error message
   - Broker address
   - SASL mechanism used
   - Timestamp of failure
3. Check for platform-wide issues

### Related Errors
- `SASL_HANDSHAKE_FAILED`
- `NETWORK_EXCEPTION`
- `SSL_HANDSHAKE_FAILED`

### See Also
- [Kafka SASL_SSL Guide](../../01-authentication-security/kafka-sasl-ssl/README.md)
- [Databricks Secrets Setup](../../01-authentication-security/kafka-sasl-ssl/databricks-integration/)
```

---

## ğŸ¯ EXECUTION ROADMAP

### Week 1-2: Foundation & Critical Patterns
- [ ] Repository structure & documentation
- [ ] Kafka SASL_SSL authentication (complete implementation)
- [ ] Schema evolution & type conflicts (complete implementation)
- [ ] MAP type flattening (document your 11-step strategy)

### Week 3-4: Architecture & Modeling
- [ ] Dimensional modeling (star schema, SCD Type 2)
- [ ] Kafka Streams + Spark integration (Java topology + Python consumer)
- [ ] Multi-plant hierarchy patterns

### Week 5-6: Performance & Deployment
- [ ] Spark UI analysis (methodology + tooling)
- [ ] Billion-record optimization (calculators + configs)
- [ ] Databricks bundles (multi-environment deployment)
- [ ] GitHub Actions CI/CD

### Week 7-8: Production Operations
- [ ] Troubleshooting playbooks (schema, auth, performance)
- [ ] Runbooks (restart, migration, incident response)
- [ ] Monitoring & alerting setup
- [ ] Real-world case studies (document your projects)

---

## ğŸ’¬ HOW TO USE THIS PROMPT

1. **Copy this entire document**
2. **Feed it to Claude** with:
   ```
   "I want to build the Manufacturing Data Platform repository from scratch.
   Let's start with Phase 1, Task 1: Repository setup.
   Create the README.md, ARCHITECTURE.md, and CONTRIBUTING.md files."
   ```
3. **Work through phases sequentially**
4. **Each task builds on previous ones**

---

## ğŸ¯ SUCCESS CRITERIA

This repository is successful when:
- [ ] Solves YOUR actual production problems
- [ ] Team members use it as reference for new pipelines
- [ ] Troubleshooting time reduced by 50%
- [ ] Onboarding time for new engineers cut in half
- [ ] Patterns prevent recurring production issues
- [ ] Codebase reflects your expertise and domain knowledge

---

## ğŸŒŸ UNIQUE VALUE PROPOSITION

This isn't another tutorial repo. This is:
- **Your playbook** for manufacturing data at scale
- **Your team's knowledge base** for production patterns
- **Your portfolio** demonstrating real-world expertise
- **Your contribution** to the data engineering community

---

**Ready to build? Feed this prompt back to me and let's start with Phase 1!**
